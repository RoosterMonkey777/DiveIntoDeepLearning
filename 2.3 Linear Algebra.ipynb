{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408326ad",
   "metadata": {},
   "source": [
    "# 2.3 Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bbe0fe",
   "metadata": {},
   "source": [
    "Can load datasets into tensors and manipulate these tensors with basic mathematical operations. To start building sophisticated models, will need a few tools from linear algebra. This section offers a gentle introduction to the most essential concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32f459",
   "metadata": {},
   "source": [
    "### 2.3.1 Scalers\n",
    "Manipulating numbers one at a time. Formally, we call these values **scalars**.\n",
    "\n",
    "For example, the temperature in Palo Alto is a balmy $72$ degrees Fahrenheit. If you wanted to convert the temperature to Celsius you would evaluate the expression $c = \\frac{5}{9}(f - 32)$, setting $f$ to $72$. In this equation, the values $5$, $9$, and $32$ are constant scalars. The variables $c$ and $f$ in general represent unknown scalars.\n",
    "\n",
    "We denote scalars by ordinary lower-cased letters (e.g., $x$, $y$, and $z$)and the space of all (continuous) *real-valued* scalars by $\\mathbb{R}$.\n",
    "\n",
    "Remember that the expression $x \\in \\mathbb{R}$\n",
    "is a formal way to say that $x$ is a real-valued scalar.\n",
    "The symbol $\\in$ (pronounced \"in\")\n",
    "denotes membership in a set.\n",
    "\n",
    "(**Scalars are implemented as tensors \n",
    "that contain only one element.**)\n",
    "Below, we assign two scalars\n",
    "and perform the familiar addition, multiplication,\n",
    "division, and exponentiation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13918f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(1.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "x+y, x-y, x*y, x/y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c0f46",
   "metadata": {},
   "source": [
    "### 2.3.2 Vectors\n",
    "\n",
    "[**you can think of a vector as a fixed-length array of scalars.**].\n",
    "Call these scalars the *elements* of the vector\n",
    "\n",
    "For example, if we were training a model to predict the risk of a loan defaulting, might associate each applicant with a vector whose components correspond to quantities like their income, length of employment, or number of previous defaults.\n",
    "\n",
    "If we were studying the risk of heart attack, each vector might represent a patient and its components might correspond to\n",
    "their most recent vital signs, cholesterol levels, minutes of exercise per day, etc.\n",
    "\n",
    "We denote vectors by bold lowercase letters, \n",
    "(e.g., $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z}$).\n",
    "\n",
    "Vectors are implemented as $1^{\\textrm{st}}$-order tensors.\n",
    "\n",
    "In general, such tensors can have arbitrary lengths,\n",
    "subject to memory limitations. Caution: in Python, as in most programming languages, vector indices start at $0$, also known as *zero-based indexing*, whereas in linear algebra subscripts begin at $1$ (one-based indexing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3f7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ac18b",
   "metadata": {},
   "source": [
    "We can refer to an element of a vector by using a subscript.\n",
    "For example, $x_2$ denotes the second element of $\\mathbf{x}$. \n",
    "Since $x_2$ is a scalar, we do not bold it.\n",
    "By default, we visualize vectors \n",
    "by stacking their elements vertically.\n",
    "\n",
    "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n",
    "\n",
    "\n",
    "Here $x_1, \\ldots, x_n$ are elements of the vector.\n",
    "Later on, we will distinguish between such *column vectors*\n",
    "and *row vectors* whose elements are stacked horizontally.\n",
    "Recall that [**we access a tensor's elements via indexing.**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33567ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24394e0",
   "metadata": {},
   "source": [
    "To indicate that a vector contains $n$ elements,\n",
    "we write $\\mathbf{x} \\in \\mathbb{R}^n$.\n",
    "\n",
    "Formally, we call $n$ the *dimensionality* of the vector.\n",
    "[**In code, this corresponds to the tensor's length**],\n",
    "accessible via Python's built-in `len` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4587b501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac05169",
   "metadata": {},
   "source": [
    "Can also access the length via the shape attribute. The shape is a tuple that indicates a tensor’s length along each axis. Tensors with just one axis have shapes with just one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69878b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa7887",
   "metadata": {},
   "source": [
    "The word “dimension” gets overloaded to mean both the number of axes and the length along a particular axis. \n",
    "\n",
    "To avoid this confusion, we use **order** to refer to the number of axes and **dimensionality** exclusively to refer to the number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817a117",
   "metadata": {},
   "source": [
    "### 2.3.3 Matrices\n",
    "\n",
    "Scalars are $0^{\\textrm{th}}$-order tensors and vectors are $1^{\\textrm{st}}$-order tensors, matrices are $2^{\\textrm{nd}}$-order tensors.\n",
    "\n",
    "Denote matrices by bold capital letters (e.g., $\\mathbf{X}$, $\\mathbf{Y}$, and $\\mathbf{Z}$), and represent them in code by tensors with two axes.\n",
    "\n",
    "The expression $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
    "indicates that a matrix $\\mathbf{A}$ contains $m \\times n$ real-valued scalars, arranged as $m$ rows and $n$ columns.\n",
    "\n",
    "When $m = n$, we say that a matrix is *square*.\n",
    "\n",
    "Visually, can illustrate any matrix as a table. To refer to an individual element,we subscript both the row and column indices, e.g.,\n",
    "$a_{ij}$ is the value that belongs to $\\mathbf{A}$'s\n",
    "$i^{\\textrm{th}}$ row and $j^{\\textrm{th}}$ column:\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n",
    "\n",
    "\n",
    "In code, we represent a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
    "by a $2^{\\textrm{nd}}$-order tensor with shape ($m$, $n$).\n",
    "[**We can convert any appropriately sized $m \\times n$ tensor \n",
    "into an $m \\times n$ matrix**] \n",
    "by passing the desired shape to `reshape`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "393d8637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3,2)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e8a9a",
   "metadata": {},
   "source": [
    "Sometimes we want to flip the axes. When exchange a matrix's rows and columns, the result is called its *transpose*.\n",
    "\n",
    "Formally, we signify a matrix $\\mathbf{A}$'s transpose by $\\mathbf{A}^\\top$ and if $\\mathbf{B} = \\mathbf{A}^\\top$, then $b_{ij} = a_{ji}$ for all $i$ and $j$.\n",
    "\n",
    "Thus, the transpose of an $m \\times n$ matrix is an $n \\times m$ matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^\\top =\n",
    "\\begin{bmatrix}\n",
    "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
    "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "In code, we can access any (**matrix's transpose**) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57872f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4],\n",
       "        [1, 3, 5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d4ea4",
   "metadata": {},
   "source": [
    "[**Symmetric matrices are the subset of square matrices\n",
    "that are equal to their own transposes:\n",
    "$\\mathbf{A} = \\mathbf{A}^\\top$.**]\n",
    "The following matrix is symmetric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69e088b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1,2,3],[2,0,4],[3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c645d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A == A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1235c5",
   "metadata": {},
   "source": [
    "Matrices are useful for representing datasets. \n",
    "\n",
    "Typically, rows correspond to individual records and columns correspond to distinct attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281736e4",
   "metadata": {},
   "source": [
    "### 2.3.4 Tensors\n",
    "\n",
    "May need to work with higher-order [**tensors**]. Tensors (**give us a generic way of describing extensions to $n^{\\textrm{th}}$-order arrays.**)\n",
    "\n",
    "Call software objects of the *tensor class* \"tensors\" precisely because they too can have arbitrary numbers of axes.\n",
    "\n",
    "Denote general tensors by capital letters with a special font face\n",
    "(e.g., $\\mathsf{X}$, $\\mathsf{Y}$, and $\\mathsf{Z}$)and their indexing mechanism (e.g., $x_{ijk}$ and $[\\mathsf{X}]_{1, 2i-1, 3}$) \n",
    "follows naturally from that of matrices.\n",
    "\n",
    "Tensors will become more important when working with images. Each image arrives as a $3^{\\textrm{rd}}$-order tensor with axes corresponding to the height, width, and *channel*. At each spatial location, the intensities of each color (red, green, and blue)\n",
    "are stacked along the channel. \n",
    "\n",
    "Furthermore, a collection of images is represented in code by a $4^{\\textrm{th}}$-order tensor, where distinct images are indexed\n",
    "along the first axis.\n",
    "\n",
    "Higher-order tensors are constructed, as were vectors and matrices,\n",
    "by growing the number of shape components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb2292f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080424a6",
   "metadata": {},
   "source": [
    "### 2.3.5 Basic Properties of Tensor Arithmetic\n",
    "\n",
    "Scalars, vectors, matrices, and higher-order tensors\n",
    "all have some handy properties. \n",
    "\n",
    "For example, elementwise operations produce outputs that have the \n",
    "same shape as their operands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37dd13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2,3)\n",
    "B = A.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98367436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8553fa32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9c57894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 0.,  2.,  4.],\n",
       "         [ 6.,  8., 10.]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A+B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac06d89",
   "metadata": {},
   "source": [
    "The [**elementwise product of two matrices\n",
    "is called their *Hadamard product***] (denoted $\\odot$).\n",
    "We can spell out the entries \n",
    "of the Hadamard product of two matrices \n",
    "$\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}$:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2ddafca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9ead9",
   "metadata": {},
   "source": [
    "Adding or multiplying a scalar and a tensor produces a result with the same shape as the original tensor. Here, each element of the tensor is added to (or multiplied by) the scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "055eb094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2 \n",
    "X = torch.arange(24).reshape(2,3,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87372074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2,  3,  4,  5],\n",
       "         [ 6,  7,  8,  9],\n",
       "         [10, 11, 12, 13]],\n",
       "\n",
       "        [[14, 15, 16, 17],\n",
       "         [18, 19, 20, 21],\n",
       "         [22, 23, 24, 25]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9997e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  2,  4,  6],\n",
       "         [ 8, 10, 12, 14],\n",
       "         [16, 18, 20, 22]],\n",
       "\n",
       "        [[24, 26, 28, 30],\n",
       "         [32, 34, 36, 38],\n",
       "         [40, 42, 44, 46]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "982faf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a*X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a192a5c",
   "metadata": {},
   "source": [
    "### 2.3.6 Reduction\n",
    "\n",
    "Often, want to calculate [**the sum of a tensor's elements.**]\n",
    "To express the sum of the elements in a vector $\\mathbf{x}$ of length $n$, write $\\sum_{i=1}^n x_i$. \n",
    "\n",
    "There is a simple function for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "375b229c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c28c9964",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a591efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor(3.))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7cbd6",
   "metadata": {},
   "source": [
    "To express [**sums over the elements of tensors of arbitrary shape**],\n",
    "simply sum over all its axes. \n",
    "\n",
    "For example, the sum of the elements of an $m \\times n$ matrix $\\mathbf{A}$ could be written $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f1164d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e4e957d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d0fd8",
   "metadata": {},
   "source": [
    "By default, invoking the sum function *reduces* a tensor along all of its axes, eventually producing a scalar.\n",
    "\n",
    "Our libraries also allow us to [**specify the axes along which the tensor should be reduced.**]\n",
    "\n",
    "To sum over all elements along the rows (axis 0), we specify `axis=0` in `sum`. Since the input matrix reduces along axis 0 to generate the output vector, this axis is missing from the shape of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99d65293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b4698a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "502cd2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db7feac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 5., 7.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511c89c",
   "metadata": {},
   "source": [
    "Specifying `axis=1` will reduce the column dimension (axis 1) by summing up elements of all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cead2039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a2fc449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3., 12.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "734a3ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b51f5f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91428c7a",
   "metadata": {},
   "source": [
    "Reducing a matrix along both rows and columns via summation is equivalent to summing up all the elements of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76f81610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1]) == A.sum()  # Same as A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98511365",
   "metadata": {},
   "source": [
    "A related quantity is the **mean**, also called the **average**.\n",
    "\n",
    "Calculate the mean by dividing the sum by the total number of elements. Because computing the mean is so common, it gets a dedicated library function that works analogously to sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1288bd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.5000), tensor(2.5000))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed07a5",
   "metadata": {},
   "source": [
    "Likewise, the function for calculating the mean can also reduce a tensor along specific axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.mean(axis=0), A.sum(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
