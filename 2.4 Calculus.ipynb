{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff317415",
   "metadata": {},
   "source": [
    "# 2.4 Calculus\n",
    " \n",
    "For a polygon with $n$ vertices,\n",
    "we obtain $n$ triangles.\n",
    "The height of each triangle approaches the radius $r$ \n",
    "as we partition the circle more finely. \n",
    "At the same time, its base approaches $2 \\pi r/n$, \n",
    "since the ratio between arc and secant approaches 1 \n",
    "for a large number of vertices. \n",
    "Thus, the area of the polygon approaches\n",
    "$n \\cdot r \\cdot \\frac{1}{2} (2 \\pi r/n) = \\pi r^2$.\n",
    "\n",
    "This limiting procedure is at the root of both \n",
    "*differential calculus* and *integral calculus*. \n",
    "The former can tell us how to increase\n",
    "or decrease a function's value by\n",
    "manipulating its arguments. \n",
    "\n",
    "This comes in handy for the *optimization problems*\n",
    "that we face in deep learning,\n",
    "where we repeatedly update our parameters \n",
    "in order to decrease the loss function.\n",
    "\n",
    "Optimization addresses how to fit our models to training data,\n",
    "and calculus is its key prerequisite.\n",
    "\n",
    "However, do not forget that our **ultimate goal\n",
    "is to perform well on *previously unseen* data**.\n",
    "That problem is called **generalization**\n",
    "and will be a key focus of other chapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3387b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905fda5",
   "metadata": {},
   "source": [
    "### 2.4.1 Derivatives and Differentiation\n",
    "A *derivative* is the rate of change in a function with respect to changes in its arguments.\n",
    "\n",
    "Derivatives can tell us how rapidly a loss function would increase or decrease were we to *increase* or *decrease* each parameter by an infinitesimally small amount.\n",
    "\n",
    "Formally, for functions $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, that map from scalars to scalars, **the *derivative* of $f$ at a point $x$ is defined as**\n",
    "\n",
    "**$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.$$**\n",
    "\n",
    "\n",
    "This term on the right hand side is called a *limit* and it tells us what happens to the value of an expression as a specified variable approaches a particular value. This limit tells us what the ratio between a perturbation $h$ and the change in the function value $f(x + h) - f(x)$ converges to as we shrink its size to zero.\n",
    "\n",
    "When $f'(x)$ exists, $f$ is said \n",
    "to be *differentiable* at $x$;\n",
    "and when $f'(x)$ exists for all $x$\n",
    "on a set, e.g., the interval $[a,b]$, \n",
    "we say that $f$ is differentiable on this set.\n",
    "Not all functions are differentiable,\n",
    "including many that we wish to optimize,\n",
    "such as accuracy and the area under the\n",
    "receiving operating characteristic (AUC).\n",
    "\n",
    "However, because computing the derivative of the loss \n",
    "is a crucial step in nearly all \n",
    "algorithms for training deep neural networks,\n",
    "we often optimize a differentiable *surrogate* instead.\n",
    "\n",
    "\n",
    "We can interpret the derivative \n",
    "$f'(x)$\n",
    "as the *instantaneous* rate of change \n",
    "of $f(x)$ with respect to $x$.\n",
    "Let's develop some intuition with an example.\n",
    "(**Define $u = f(x) = 3x^2-4x$.**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8085baf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x\n",
    "f(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b686b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical limit=2.30000\n",
      "h=0.01000, numerical limit=2.03000\n",
      "h=0.00100, numerical limit=2.00300\n",
      "h=0.00010, numerical limit=2.00030\n",
      "h=0.00001, numerical limit=2.00003\n"
     ]
    }
   ],
   "source": [
    "for h in 10.0**np.arange(-1, -6, -1):\n",
    "    print(f'h={h:.5f}, numerical limit={(f(1+h)-f(1))/h:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525c186",
   "metadata": {},
   "source": [
    "This code appears to be performing an iterative calculation to estimate the derivative of a function f(x) at a specific point, x = 1, using numerical approximation techniques. Specifically, it seems to be using finite differences to approximate the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab840aba",
   "metadata": {},
   "source": [
    "There are several equivalent notational conventions for derivatives. Given $y = f(x)$, the following expressions are equivalent:\n",
    "\n",
    "$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n",
    "\n",
    "where the symbols $\\frac{d}{dx}$ and $D$ are *differentiation operators*.\n",
    "Below, we present the derivatives of some common functions:\n",
    "\n",
    "$$\\begin{aligned} \\frac{d}{dx} C & = 0 && \\textrm{for any constant $C$} \\\\ \\frac{d}{dx} x^n & = n x^{n-1} && \\textrm{for } n \\neq 0 \\\\ \\frac{d}{dx} e^x & = e^x \\\\ \\frac{d}{dx} \\ln x & = x^{-1}. \\end{aligned}$$\n",
    "\n",
    "Functions composed from differentiable functions \n",
    "are often themselves differentiable.\n",
    "The following rules come in handy \n",
    "for working with compositions \n",
    "of any differentiable functions \n",
    "$f$ and $g$, and constant $C$.\n",
    "\n",
    "$$\\begin{aligned} \\frac{d}{dx} [C f(x)] & = C \\frac{d}{dx} f(x) && \\textrm{Constant multiple rule} \\\\ \\frac{d}{dx} [f(x) + g(x)] & = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x) && \\textrm{Sum rule} \\\\ \\frac{d}{dx} [f(x) g(x)] & = f(x) \\frac{d}{dx} g(x) + g(x) \\frac{d}{dx} f(x) && \\textrm{Product rule} \\\\ \\frac{d}{dx} \\frac{f(x)}{g(x)} & = \\frac{g(x) \\frac{d}{dx} f(x) - f(x) \\frac{d}{dx} g(x)}{g^2(x)} && \\textrm{Quotient rule} \\end{aligned}$$\n",
    "\n",
    "Using this, we can apply the rules \n",
    "to find the derivative of $3 x^2 - 4x$ via\n",
    "\n",
    "$$\\frac{d}{dx} [3 x^2 - 4x] = 3 \\frac{d}{dx} x^2 - 4 \\frac{d}{dx} x = 6x - 4.$$\n",
    "\n",
    "Plugging in $x = 1$ shows that, indeed, \n",
    "the derivative equals $2$ at this location. \n",
    "Note that derivatives tell us \n",
    "the *slope* of a function \n",
    "at a particular location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2322d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
