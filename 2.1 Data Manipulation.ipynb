{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a0ba04",
   "metadata": {},
   "source": [
    "# 2. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735a13b",
   "metadata": {},
   "source": [
    "Will need a few skills:\n",
    "- techniques for storing and manipulating data\n",
    "- libraries for ingesting and preprocessing data from variety of sources\n",
    "- knowledge of basic linear algebraic operations that can be applied to high-dimensional data\n",
    "- enough calculus to determine which direction to adjust each parameter in order to decrease loss function\n",
    "- ability to automatically compute derivatives \n",
    "- fluency in probability (primary language for reasoning under uncertainty\n",
    "- apptitude for finding answers in documentation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b91281",
   "metadata": {},
   "source": [
    "### 2.1 Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3738fb3",
   "metadata": {},
   "source": [
    "Need ways to manipulate data, generally two important tasks:\n",
    "- acquire data\n",
    "- process once its inside the computer\n",
    "\n",
    "n-dimensional arrays are known as tensors\n",
    "\n",
    "All modern deep learning framworks, tensor class resembles NumPy's ndarray class (with more features added)\n",
    "\n",
    "- Tensor class supports automatic differntiation\n",
    "- Leverages GPUs to accerlate numerical computation, whereas NumPy only runs on CPU \n",
    "\n",
    "These properties make neural networks easy to code and fast to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ccd737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cafa73",
   "metadata": {},
   "source": [
    "A tensor represents array of numerical values (in any dimensions)\n",
    "\n",
    "One-dimensional tensors are known as **vectors**\n",
    "\n",
    "Two-dimensional tensors are known as **matrix**\n",
    "\n",
    "K-dimensional, is known as a k-th order tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc78371",
   "metadata": {},
   "source": [
    "Pytorch offers many ways to create new tensors and prepopulating with values. Example arange(n), will create evenly spaced tensors starting at 0 (included) up to n (not included)\n",
    "\n",
    "Default, the interval size is 1 (can be changed)\n",
    "\n",
    "New tensors are stored in main memory and designated for CPU-based computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cdb0d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bf864b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.arange(10, dtype=torch.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d48a2",
   "metadata": {},
   "source": [
    "Values are called **elements** of the tensor\n",
    "\n",
    "tensor x contains 12 elements\n",
    "tensor y contains 10 elements\n",
    "\n",
    "Can inspect the total number of elements in a tensor using **numel** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a73aec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a9f810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e49cf",
   "metadata": {},
   "source": [
    "Can also see the shape of the tensor (length along each direction)\n",
    "\n",
    "Because x is a vector, it will only have a single dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94efaef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e248a7",
   "metadata": {},
   "source": [
    "Can change the shape of a tensor without altering its size or values, by invoking **reshape** function \n",
    "\n",
    "For example, we can transform our vector x whose shape is (12,) to a matrix X with shape (3, 4). This new tensor retains all elements but reconfigures them into a matrix. Notice that the elements of our vector are laid out one row at a time and thus x[3] == X[0, 3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c59994b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24501df",
   "metadata": {},
   "source": [
    "Note: specifying every shape component to reshape is redundant. Because we already know our tensorâ€™s size, we can work out one component of the shape given the rest. \n",
    "\n",
    "To automatically infer one component of the shape, we can place a -1 for the shape component that should be inferred automatically. In our case, instead of calling x.reshape(3, 4), we could have equivalently called x.reshape(-1, 4) or x.reshape(3, -1).\n",
    "\n",
    "Practitioners often need to work with tensors initialized to contain all 0s or 1s. We can construct a tensor with all elements set to 0 and a shape of (2, 3, 4) via the zeros function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a23b59f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "114db173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2,4,3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a094074c",
   "metadata": {},
   "source": [
    "Similarly, we can create a tensor with all 1s by invoking ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85829edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2,3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ce016",
   "metadata": {},
   "source": [
    "We often wabt to sample each element randomly (and independently) from a given probability distribution. \n",
    "\n",
    "For example, the **parameters of neural networks are often initialized randomly**. \n",
    "\n",
    "The following snippet creates a tensor with elements drawn from a standard Gaussian (normal) distribution with mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bc4885d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7670,  0.5252, -0.9545,  0.1566],\n",
       "        [-1.5705,  0.8614, -0.2079, -0.4664],\n",
       "        [-0.8039, -1.8453, -0.0884,  1.0441]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d71ba",
   "metadata": {},
   "source": [
    "Finally, we can construct tensors by supplying the exact values for each element by supplying (possibly nested) Python list(s) containing numerical literals. \n",
    "\n",
    "Here, we construct a matrix with a list of lists, where the outermost list corresponds to axis 0, and the inner list corresponds to axis 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5faa841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 3, 4],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2,1,3,4], [1,2,3,4], [4,3,2,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ffc19",
   "metadata": {},
   "source": [
    "### 2.1.2 Indexing and Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191750db",
   "metadata": {},
   "source": [
    "As with Python lists, can access tensor elements by indexing (starting with 0). \n",
    "\n",
    "To access an element based on its position relative to the end of the list, we can use negative indexing. \n",
    "\n",
    "We can access whole ranges of indices via slicing (e.g., X[start:stop]), where the returned value includes the first index (start) but not the last (stop). \n",
    "\n",
    "When only one index (or slice) is specified for a kth-order tensor, it is applied along axis 0. Thus, in the following code, [-1] selects the last row and [1:3] selects the second and third rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fa39868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  9, 10, 11])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6523e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a9ae7",
   "metadata": {},
   "source": [
    "Beyond reading them, we can also write elements of a matrix by specifying indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "169f5101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13, 13, 13, 13],\n",
       "        [13, 13, 17, 13],\n",
       "        [13, 13, 13, 13]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbac5446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13, 13, 13, 13],\n",
       "        [13, 13, 17, 13],\n",
       "        [13, 13, 13, 13]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1,2]= 17 # should change the 6 to a 17\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a8c75",
   "metadata": {},
   "source": [
    "If we want to assign multiple elements the same value, we apply the indexing on the left-hand side of the assignment operation. For instance, [:2, :] accesses the first and second rows, where : takes all the elements along axis 1 (column). While we discussed indexing for matrices, this also works for vectors and for tensors of more than two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60c7009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 12, 12, 12],\n",
       "        [12, 12, 12, 12],\n",
       "        [13, 13, 13, 13]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2,:] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "130d3abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8, 12, 12, 12],\n",
       "        [ 8, 12, 12, 12],\n",
       "        [13, 13, 13, 13]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2,:1] = 8\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d31ab46",
   "metadata": {},
   "source": [
    "### 2.1.3 Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad943b",
   "metadata": {},
   "source": [
    "Now that we know how to construct tensors and how to read from and write to their elements, we can begin to manipulate them with various mathematical operations\n",
    "\n",
    "Among the most useful of these are the elementwise operations.\n",
    "\n",
    "These apply a standard scalar operation to each element of a tensor\n",
    "\n",
    "For functions that take two tensors as inputs, elementwise operations apply some standard binary operator on each pair of corresponding elements. We can create an elementwise function from any function that maps from a scalar to a scalar.\n",
    "\n",
    "In mathematical notation, we denote such unary scalar operators (taking one input) by the signature:\n",
    "f: R -> R, function maps from any real number onto some other real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a41675d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2980.9580, 162754.7969, 162754.7969, 162754.7969,   2980.9580,\n",
       "        162754.7969, 162754.7969, 162754.7969, 442413.4062, 442413.4062,\n",
       "        442413.4062, 442413.4062])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dfc9704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 12, 12, 12,  8, 12, 12, 12, 13, 13, 13, 13])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e9552",
   "metadata": {},
   "source": [
    "torch.exp(x) function computes the element-wise exponential of the input tensor x. It applies the exponential function, e^x, to each element of the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c74648",
   "metadata": {},
   "source": [
    "Likewise, we denote *binary* scalar operators,\n",
    "which map pairs of real numbers\n",
    "to a (single) real number\n",
    "via the signature \n",
    "$f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "Given any two vectors $\\mathbf{u}$ \n",
    "and $\\mathbf{v}$ *of the same shape*,\n",
    "and a binary operator $f$, we can produce a vector\n",
    "$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$\n",
    "by setting $c_i \\gets f(u_i, v_i)$ for all $i$,\n",
    "where $c_i, u_i$, and $v_i$ are the $i^\\textrm{th}$ elements\n",
    "of vectors $\\mathbf{c}, \\mathbf{u}$, and $\\mathbf{v}$.\n",
    "Here, we produced the vector-valued\n",
    "$F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n",
    "by *lifting* the scalar function\n",
    "to an elementwise vector operation.\n",
    "The common standard arithmetic operators\n",
    "for addition (`+`), subtraction (`-`), \n",
    "multiplication (`*`), division (`/`), \n",
    "and exponentiation (`**`)\n",
    "have all been *lifted* to elementwise operations\n",
    "for identically-shaped tensors of arbitrary shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fd51385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2,4,8])\n",
    "y = torch.tensor([2,2,2,2])\n",
    "x+y ,x-y ,x*y ,x/y ,x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecac3f0",
   "metadata": {},
   "source": [
    "In addition to elementwise computations, we can also perform linear algebraic operations, such as dot products and matrix multiplications. We will elaborate on these in Section 2.3\n",
    "\n",
    "Can also concatenate multiple tensors, stacking them end-to-end to form a larger one. \n",
    "\n",
    "Just need to provide a list of tensors and tell the system along which axis to concatenate. The example below shows what happens when we concatenate two matrices along rows (axis 0) instead of columns (axis 1). \n",
    "\n",
    "We can see that the first output's axis-0 length ($6$) is the sum of the two input tensors' axis-0 lengths ($3 + 3$); while the second output's axis-1 length ($8$) is the sum of the two input tensors' axis-1 lengths ($4 + 4$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8d9993f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ad14d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 4., 3.],\n",
       "        [1., 2., 3., 4.],\n",
       "        [4., 3., 2., 1.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4c87c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 2.,  1.,  4.,  3.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((X,Y), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc929b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((X,Y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d3f7b5",
   "metadata": {},
   "source": [
    "Sometimes, we want to construct a binary tensor via logical statements. Take X == Y as an example. For each position i, j, if X[i, j] and Y[i, j] are equal, then the corresponding entry in the result takes value 1, otherwise it takes value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d514411d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b66111",
   "metadata": {},
   "source": [
    "Summing all the elements in the tensor yields a tensor with only one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "108bcaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9325c",
   "metadata": {},
   "source": [
    "### 2.1.4 Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10987c5c",
   "metadata": {},
   "source": [
    "You know how to perform elementwise binary operations on two tensors of the same shape. \n",
    "\n",
    "Under certain conditions, even when shapes differ, can still perform elementwise binary operations by invoking the **broadcasting mechanism**. \n",
    "\n",
    "Broadcasting works according to the following two-step procedure: \n",
    "- (i) expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape; \n",
    "- (ii) perform an elementwise operation on the resulting arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9107fec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3,1))\n",
    "b = torch.arange(2).reshape((1,2))\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830afae0",
   "metadata": {},
   "source": [
    "Since a and b are 3 x 1 and 2 x 1 matrices, respectively, their shapes do not match up. Broadcasting produces a larger matrix by replicating matrix a along the columns and matrix b along the rows before adding them elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48bed94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb881a34",
   "metadata": {},
   "source": [
    "### 2.1.5 Saving Memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a142870",
   "metadata": {},
   "source": [
    "[**Running operations can cause new memory to be\n",
    "allocated to host results.**]\n",
    "\n",
    "For example, if we write `Y = X + Y`, we dereference the tensor that `Y` used to point to and instead point `Y` at the newly allocated memory.\n",
    "\n",
    "\n",
    "We can demonstrate this issue with Python's `id()` function, which gives us the exact address of the referenced object in memory. Note that after we run `Y = Y + X`,`id(Y)` points to a different location.\n",
    "\n",
    "That is because Python first evaluates `Y + X`,allocating new memory for the result and then points `Y` to this new location in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26a5da2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y+X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf7dd4",
   "metadata": {},
   "source": [
    "This is undesirable for two reasons.\n",
    "\n",
    "First, we do not want to run around allocating memory unnecessarily all the time.\n",
    "In machine learning, we often have hundreds of megabytes of parameters and update all of them multiple times per second. Whenever possible, we want to perform these updates *in place*.\n",
    "\n",
    "Second, we might point at the same parameters from multiple variables. If we do not update in place, we must be careful to update all of these references,lest we spring a memory leak or inadvertently refer to stale parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb61ea9",
   "metadata": {},
   "source": [
    "Fortunately, (**performing in-place operations**) is easy.\n",
    "We can assign the result of an operation to a previously allocated array `Y` by using slice notation: `Y[:] = <expression>`.To illustrate this concept, we overwrite the values of tensor `Z`, after initializing it, using `zeros_like`,to have the same shape as `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9384b0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 140284132706640\n",
      "id(Z): 140284132706640\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d720ff",
   "metadata": {},
   "source": [
    "[**If the value of `X` is not reused in subsequent computations,\n",
    "we can also use `X[:] = X + Y` or `X += Y`\n",
    "to reduce the memory overhead of the operation.**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "054f4e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "746062d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  3.,  8.,  9.],\n",
       "        [ 9., 12., 15., 18.],\n",
       "        [20., 21., 22., 23.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da57c3a",
   "metadata": {},
   "source": [
    "### 2.1.6 Conversion to Other Python Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c928ee",
   "metadata": {},
   "source": [
    "Converting to a NumPy tensor (ndarray), or vice versa, is easy. \n",
    "\n",
    "The torch tensor and NumPy array will share their underlying memory, and changing one through an in-place operation will also change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ccf12cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.from_numpy(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9ec43",
   "metadata": {},
   "source": [
    "A = X.numpy():\n",
    "- This line converts a PyTorch tensor X into a NumPy array A.\n",
    "- The method numpy() is a method available for PyTorch tensors that converts them into NumPy arrays.\n",
    "- After this line, A contains the same data as X, but represented as a NumPy array.\n",
    "\n",
    "B = torch.from_numpy(A):\n",
    "- This line converts a NumPy array A into a PyTorch tensor B.\n",
    "- The function torch.from_numpy() creates a PyTorch tensor from a NumPy array.\n",
    "- After this line, B contains the same data as A, but represented as a PyTorch tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb6e21",
   "metadata": {},
   "source": [
    "To convert a size-1 tensor to a Python scalar, we can invoke the item function or Pythonâ€™s built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61186978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d96c78",
   "metadata": {},
   "source": [
    "### 2.1.7 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499601a",
   "metadata": {},
   "source": [
    "The tensor class is the main interface for storing and manipulating data in deep learning libraries. Tensors provide a variety of functionalities including construction routines; indexing and slicing; basic mathematics operations; broadcasting; memory-efficient assignment; and conversion to and from other Python objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25aacf",
   "metadata": {},
   "source": [
    "### 2.1.8 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6d9c0",
   "metadata": {},
   "source": [
    "1. Run the code in this section. Change the conditional statement X == Y to X < Y or X > Y, and then see what kind of tensor you can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41c1ccc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  3.,  8.,  9.],\n",
       "        [ 9., 12., 15., 18.],\n",
       "        [20., 21., 22., 23.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62293299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  2.,  6.,  6.],\n",
       "        [ 5.,  7.,  9., 11.],\n",
       "        [12., 12., 12., 12.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68e59b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f33248c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X < Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a1caa6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X > Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df1bff",
   "metadata": {},
   "source": [
    "2. Replace the two tensors that operate by element in the broadcasting mechanism with other shapes, e.g., 3-dimensional tensors. Is the result the same as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f6d6ba04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0]],\n",
       " \n",
       "         [[1]],\n",
       " \n",
       "         [[2]]]),\n",
       " tensor([[[0],\n",
       "          [1]]]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.arange(3).reshape((3,1,1))\n",
    "d = torch.arange(2).reshape((1,2,1))\n",
    "c,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a56233e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0],\n",
       "         [1]],\n",
       "\n",
       "        [[1],\n",
       "         [2]],\n",
       "\n",
       "        [[2],\n",
       "         [3]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89a1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
