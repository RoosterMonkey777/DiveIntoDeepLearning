{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b802e7ab",
   "metadata": {},
   "source": [
    "# 2.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f46354",
   "metadata": {},
   "source": [
    "Have been working with synthetic data that arrived in ready-made tensors. \n",
    "\n",
    "However, to apply deep learning in the wild we must extract messy data stored in arbitrary formats, and preprocess it to suit our needs. Fortunately, the **pandas** library can do much of the heavy lifting. \n",
    "\n",
    "This section, while no substitute for a proper pandas tutorial, will give you a crash course on some of the most common routines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7a7c3",
   "metadata": {},
   "source": [
    "### 2.2.1 Reading the Dataset\n",
    "\n",
    "- Comma-separated values (CSV) files are ubiquitous for the storing of tabular (spreadsheet-like) data. \n",
    "\n",
    "- Each line corresponds to one record and consists of several (comma-separated) fields \n",
    "\n",
    "- e.g., “Albert Einstein,March 14 1879,Ulm,Federal polytechnic school,field of gravitational physics”. \n",
    "\n",
    "- To demonstrate how to load CSV files with pandas, we create a CSV file below . ./data/house_tiny.csv. \n",
    "\n",
    "- This file represents a dataset of homes, where each row corresponds to a distinct home and the columns correspond to the number of rooms (NumRooms), the roof type (RoofType), and the price (Price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "075fdfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('''\n",
    "    NumRooms,RoofType,Price\n",
    "    NA,NA,127500\n",
    "    2,NA,106000\n",
    "    4,Slate,178100\n",
    "    NA,NA,140000''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a0278",
   "metadata": {},
   "source": [
    "This code snippet creates a CSV file named house_tiny.csv in a directory named 'data' (which is created if it doesn't already exist) in the parent directory of the current working directory.\n",
    "\n",
    "- **import os**: Imports the Python os module, which provides a way to interact with the operating system.\n",
    "\n",
    "- **os.makedirs(os.path.join('..', 'data'), exist_ok=True)**: Creates a directory named data in the parent directory of the current working directory. The os.makedirs() function is used for creating directories recursively. The exist_ok=True argument ensures that the function doesn't raise an error if the directory already exists.\n",
    "\n",
    "- **data_file = os.path.join('..', 'data', 'house_tiny.csv')**: Constructs the file path for the CSV file. It concatenates the directory path ../data with the filename house_tiny.csv using os.path.join().\n",
    "\n",
    "- **with open(data_file, 'w') as f:**: Opens the CSV file house_tiny.csv in write mode. The with statement ensures that the file is properly closed after writing. It assigns the file object to the variable f.\n",
    "\n",
    "- **f.write(''' ... ''')**: Writes the content enclosed in triple single quotes to the CSV file. The content consists of four lines, each representing a row in the CSV file. Each row contains values separated by commas: NumRooms, RoofType, and Price. The last character before the closing triple quotes is a newline character, ensuring that each row is written on a new line in the file.\n",
    "\n",
    "The first row contains column headers.\n",
    "The subsequent rows represent data entries for the respective columns.\n",
    "\n",
    "\n",
    "In summary, this code snippet creates a CSV file named house_tiny.csv with sample data representing the number of rooms (NumRooms), roof type (RoofType), and price (Price) of houses. The file is created in a directory named data in the parent directory of the current working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c948b96",
   "metadata": {},
   "source": [
    "import pandas and load the dataset with read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693accf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      NumRooms RoofType   Price\n",
      "0           NA      NaN  127500\n",
      "1            2      NaN  106000\n",
      "2            4    Slate  178100\n",
      "3           NA      NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7bef51",
   "metadata": {},
   "source": [
    "### 2.2 Data Preperation\n",
    "\n",
    "In supervised learning, we train models to predict a designated *target* value, given some set of *input* values. \n",
    "\n",
    "First step in processing the dataset is to separate out columns corresponding to input versus target values. \n",
    "We can select columns either by name or via **integer-location based indexing (`iloc`)**.\n",
    "\n",
    "You might have noticed that `pandas` replaced all CSV entries with value `NA` with a special `NaN` (*not a number*) value. \n",
    "\n",
    "This can also happen whenever an entry is empty,e.g., \"3,,,270000\".\n",
    "These are called *missing values* and they are a persistent menace that you will confront throughout your career. \n",
    "\n",
    "Depending upon the context, missing values might be handled either via **imputation** or **deletion**.\n",
    "\n",
    "Imputation replaces missing values with estimates of their values while deletion simply discards either those rows or those columns that contain missing values. \n",
    "\n",
    "Here are some common imputation heuristics:\n",
    "\n",
    "[**For categorical input fields, we can treat `NaN` as a category.**]\n",
    "\n",
    "Since the `RoofType` column takes values `Slate` and `NaN`,\n",
    "`pandas` can convert this column into two columns `RoofType_Slate` and `RoofType_nan`.\n",
    "\n",
    "A row whose roof type is `Slate` will set values \n",
    "of `RoofType_Slate` and `RoofType_nan` to 1 and 0, respectively.\n",
    "\n",
    "The converse holds for a row with a missing `RoofType` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c943ce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       NumRooms_    2      NumRooms_    4      NumRooms_    NA  \\\n",
      "0               False               False                 True   \n",
      "1                True               False                False   \n",
      "2               False                True                False   \n",
      "3               False               False                 True   \n",
      "\n",
      "       NumRooms_nan  RoofType_Slate  RoofType_nan  \n",
      "0             False           False          True  \n",
      "1             False           False          True  \n",
      "2             False            True         False  \n",
      "3             False           False          True  \n"
     ]
    }
   ],
   "source": [
    "inputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2b0b5",
   "metadata": {},
   "source": [
    "- **inputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]**: \n",
    "    - This line extracts the input features and the target variable from the DataFrame 'data'.\n",
    "    - **data.iloc[:, 0:2]** selects all rows and the first two columns of the DataFrame, representing the input features.\n",
    "    - **data.iloc[:, 2]** selects all rows of the DataFrame and the third column, representing the target variable.\n",
    "    - The extracted input features are assigned to the variable inputs, and the target variable is assigned to the variable targets.\n",
    "\n",
    "- **inputs = pd.get_dummies(inputs, dummy_na=True)**:\n",
    "    - This line creates dummy variables for categorical features in the inputs DataFrame using one-hot encoding.\n",
    "    - **pd.get_dummies()** is a Pandas function used for one-hot encoding categorical variables.\n",
    "    - By specifying **dummy_na=True**, it also creates dummy variables for missing values (NaNs) in the input data.\n",
    "    - The resulting DataFrame with dummy variables is assigned back to the variable inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f963667",
   "metadata": {},
   "source": [
    "For missing numerical values, \n",
    "one common heuristic is to \n",
    "[**replace the `NaN` entries with \n",
    "the mean value of the corresponding column**].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f2530b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       NumRooms_    2      NumRooms_    4      NumRooms_    NA  \\\n",
      "0               False               False                 True   \n",
      "1                True               False                False   \n",
      "2               False                True                False   \n",
      "3               False               False                 True   \n",
      "\n",
      "       NumRooms_nan  RoofType_Slate  RoofType_nan  \n",
      "0             False           False          True  \n",
      "1             False           False          True  \n",
      "2             False            True         False  \n",
      "3             False           False          True  \n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297aa2a7",
   "metadata": {},
   "source": [
    "output should look like this after imputation\n",
    "   - Index,      NumRooms,      RoofType_Slate,      RoofType_nan,\n",
    "- 0,       3.0,           False,          True\n",
    "- 1,       2.0,           False ,         True\n",
    "- 2 ,      4.0   ,         True   ,      False\n",
    "- 3 ,      3.0  ,         False ,         True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc990d17",
   "metadata": {},
   "source": [
    "### 2.2.3 Conversion to the Tensor Format\n",
    "Now that all the entries in inputs and targets are numerical, we can load them into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb8b4e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 1., 0., 0., 1.],\n",
       "         [1., 0., 0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500., 106000., 178100., 140000.], dtype=torch.float64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor(inputs.to_numpy(dtype=float))\n",
    "y = torch.tensor(targets.to_numpy(dtype=float))\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e85dbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a61d30",
   "metadata": {},
   "source": [
    "- **X = torch.tensor(inputs.to_numpy(dtype=float))**:\n",
    "    - **inputs.to_numpy(dtype=float)** converts the DataFrame inputs to a NumPy array with data type float.\n",
    "    - **torch.tensor()** converts the NumPy array to a PyTorch tensor X.\n",
    "\n",
    "- **y = torch.tensor(targets.to_numpy(dtype=float))**:\n",
    "    - **targets.to_numpy(dtype=float)** converts the DataFrame targets to a NumPy array with data type float.\n",
    "    - **torch.tensor()** converts the NumPy array to a PyTorch tensor y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed12f74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs.to_numpy(dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db08bc",
   "metadata": {},
   "source": [
    "### 2.2.4 Discussion\n",
    "\n",
    "Now know how to partition data columns, impute missing variables, \n",
    "and load `pandas` data into tensors. \n",
    " \n",
    "While this crash course kept things simple, data processing can get hairy.\n",
    "\n",
    "For example, rather than arriving in a single CSV file, our dataset might be spread across multiple files extracted from a relational database.\n",
    "\n",
    "For instance, in an e-commerce application, customer addresses might live in one table and purchase data in another.\n",
    "\n",
    "Moreover, practitioners face myriad data types beyond categorical and numeric, for example, text strings, images, audio data, and point clouds. \n",
    "\n",
    "Oftentimes, advanced tools and efficient algorithms are required in order to prevent data processing from becoming the biggest bottleneck in the machine learning pipeline. \n",
    "\n",
    "We must pay attention to data quality. Real-world datasets are often plagued by outliers, faulty measurements from sensors, and recording errors, which must be addressed before feeding the data into any model. \n",
    "\n",
    "Data visualization tools such as [seaborn](https://seaborn.pydata.org/), \n",
    "[Bokeh](https://docs.bokeh.org/), or [matplotlib](https://matplotlib.org/)\n",
    "can help you to manually inspect the data and develop intuitions about \n",
    "the type of problems you may need to address."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24123547",
   "metadata": {},
   "source": [
    "### 2.2.3 Exercises\n",
    "\n",
    "1. Try loading datasets, e.g., Abalone from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php) and inspect their properties. What fraction of them has missing values? What fraction of the variables is numerical, categorical, or text?\n",
    "1. Try indexing and selecting data columns by name rather than by column number. The pandas documentation on [indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) has further details on how to do this.\n",
    "1. How large a dataset do you think you could load this way? What might be the limitations? Hint: consider the time to read the data, representation, processing, and memory footprint. Try this out on your laptop. What happens if you try it out on a server? \n",
    "1. How would you deal with data that has a very large number of categories? What if the category labels are all unique? Should you include the latter?\n",
    "1. What alternatives to pandas can you think of? How about [loading NumPy tensors from a file](https://numpy.org/doc/stable/reference/generated/numpy.load.html)? Check out [Pillow](https://python-pillow.org/), the Python Imaging Library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce221ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
